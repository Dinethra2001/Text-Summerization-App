# -*- coding: utf-8 -*-
"""Copy of irwa orginal.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1G4mAXCLjUt-vr6Cxf6Q_aIuiHydyp5Zv
"""


"""# Load th dataset"""

import requests
from bs4 import BeautifulSoup
import re
from transformers import BartTokenizer, BartForConditionalGeneration, pipeline
import nltk
from nltk.corpus import stopwords
from nltk.stem.wordnet import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer
import gensim
from gensim import corpora
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords


# Function to fetch and extract text from a URL
def fetch_text_from_url(url):
    try:
        response = requests.get(url)
        response.raise_for_status()  # Raise an error for bad responses
        soup = BeautifulSoup(response.text, 'html.parser')

        # Extract text from paragraphs
        paragraphs = soup.find_all('p')
        text = ' '.join([para.get_text() for para in paragraphs])
        return text

    except Exception as e:
        print(f"Error fetching the URL: {e}")
        return ""

# Text Summarization Preprocessing
def preprocess_text_for_summarization(text):
    text = text.lower()
    text = re.sub(r'[^a-zA-Z\s\.]', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

# Load BART tokenizer and model for summarization
summarizer_tokenizer = BartTokenizer.from_pretrained("facebook/bart-large-cnn",truncation=True, padding=True ,device=0)
summarizer_model = BartForConditionalGeneration.from_pretrained("facebook/bart-large-cnn")

# Function to summarize input text
def summarize_text(text, max_len,pval):

    print(" summarize_text function is called")
    if pval == 0:
          # Fetch the text from the provided URL
        text = fetch_text_from_url(text)

    processed_text = preprocess_text_for_summarization(text)

    inputs = summarizer_tokenizer.encode("summarize: " + processed_text, return_tensors="pt", max_length=512, truncation=True)
    summary_ids = summarizer_model.generate(inputs, max_length=max_len, min_length=40, length_penalty=2.0, num_beams=6, early_stopping=True)
    summary = summarizer_tokenizer.decode(summary_ids[0], skip_special_tokens=True)


    return processed_text, summary


#sentiment analysis

# Function to predict sentiment for the generated summary
def predict_sentiment_hf(text):

    print(" predict_sentiment_hf function is called")
    """
    Predict the sentiment using a pre-trained Hugging Face model.
    Input: text (str): The summarized text for sentiment analysis.
    Output: Sentiment label (str): Positive or Negative and confidence score (float).
    """
    # Load the pre-trained sentiment analysis pipeline
    sentiment_pipeline = pipeline("sentiment-analysis", model="distilbert-base-uncased-finetuned-sst-2-english" ,device=0)

    # Use the pipeline to predict sentiment
    result = sentiment_pipeline(text)

    # Extract the label (POSITIVE/NEGATIVE) and score
    sentiment = result[0]['label']
    confidence = result[0]['score']
    confidence  = f"{round(confidence * 100, 2)}%"
    return sentiment, confidence




## Keyword handling

# Initialize the lemmatizer and stop words
lmtr = WordNetLemmatizer()
Stopwords = set(stopwords.words('english'))

# Create custom stop words
new_stop_words = ["fig", "figure", "image", "sample", "using", "show", "result",
                  "large", "also", "one", "two", "three", "four", "five",
                  "seven", "eight", "nine"]

# Create a set of stop words using union
stop_words = Stopwords.union(set(new_stop_words))

def preprocess_text(txt):

    print("preprocess_text func is called")
    # Lower case
    txt = txt.lower()
    # Remove HTML tags
    txt = re.sub(r"<.*?>", " ", txt)
    # Remove special characters and digits
    txt = re.sub(r"[^a-zA-Z]", " ", txt)
    # Tokenization
    txt = nltk.word_tokenize(txt)
    # Remove stopwords
    txt = [word for word in txt if word not in stop_words]
    # Remove words less than three letters
    txt = [word for word in txt if len(word) >= 3]
    # Lemmatize
    txt = [lmtr.lemmatize(word) for word in txt]

    return " ".join(txt)

def extract_keywords(user_input):

    # Preprocess the user input
    preprocessed_text = preprocess_text(user_input)
    # Create a list containing the preprocessed input for TF-IDF
    docs = [preprocessed_text]
    # Initialize the TF-IDF vectorizer
    vectorizer = TfidfVectorizer(max_features=15)  # Extract top k keywords based on TF-IDF
    # Fit and transform the document into the TF-IDF matrix
    X = vectorizer.fit_transform(docs)
    # Get the feature names (i.e., the words)
    keywords = vectorizer.get_feature_names_out()
    # Get the TF-IDF scores
    tfidf_scores = X.toarray()
    # Extract the top keywords
    keyword_score_pairs = list(zip(keywords, tfidf_scores[0]))
    sorted_keywords = sorted(keyword_score_pairs, key=lambda x: x[1], reverse=True)
    # Extract the top 10 keywords
    top_keywords = [keyword for keyword, _ in sorted_keywords[:10]]

    return top_keywords



#topic hadling

def topic_modeling(text):

    print("topic_modeling func is called")
    # Preprocess the text
    stop_words = set(stopwords.words('english'))
    tokens = word_tokenize(text.lower())
    tokens = [word for word in tokens if word.isalnum() and word not in stop_words]

    # Create a dictionary and corpus
    dictionary = corpora.Dictionary([tokens])
    corpus = [dictionary.doc2bow(tokens)]

    # Create the LDA model
    lda_model = gensim.models.LdaModel(corpus, num_topics=1, id2word=dictionary, passes=15)

    # Get the most representative topic
    topics = lda_model.print_topics(num_words=4)
    
    # Extract only the most prominent topic
    most_representative_topic = topics[0]  # Get the first topic

    # Extract words from the most prominent topic
    topic_words = most_representative_topic[1]  # Get the words and their weights
    words = [word.split('*')[1].strip().strip('"') for word in topic_words.split('+')]  # Extract just the words

    return words


